{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdudgJZZQ37qh6e0W9owlr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zmohaghegh/Zahra-PDF-Chat-RAG/blob/main/Zahra_RAG_Chatbot_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community langchain-text-splitters pypdf sentence-transformers faiss-cpu transformers gradio\n",
        "import torch\n",
        "import gradio as gr\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ---\n",
        "# ØªØ´Ø®ÛŒØµ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± (GPU ÛŒØ§ CPU)\n",
        "device_id = 0 if torch.cuda.is_available() else -1\n",
        "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Running on: {device_name}\")\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Embedding (Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ø³Ø¨Ú©)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': device_name}\n",
        ")\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡Ù†Ø¯Ù‡ (Flan-T5)\n",
        "print(\"Loading LLM model...\")\n",
        "qa_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    device=device_id\n",
        ")\n",
        "\n",
        "# --- ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø³ÛŒØ³ØªÙ… ---\n",
        "\n",
        "def process_pdf_and_chat(file_obj, user_query):\n",
        "    \"\"\"\n",
        "    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ÙØ§ÛŒÙ„ PDF Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù‡ØŒ Ø¢Ù† Ø±Ø§ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø³Ù¾Ø³ Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
        "    \"\"\"\n",
        "    if file_obj is None:\n",
        "        return \"Ø²Ù‡Ø±Ø§ Ø¬Ø§Ù†ØŒ Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÛŒÚ© ÙØ§ÛŒÙ„ PDF Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†.\"\n",
        "    if not user_query:\n",
        "        return \"Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯.\"\n",
        "\n",
        "    try:\n",
        "        # Û±. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ PDF\n",
        "        loader = PyPDFLoader(file_obj.name)\n",
        "        pages = loader.load()\n",
        "\n",
        "        # Û². Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙ† (Chunking) Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ù‡ØªØ±\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        docs = text_splitter.split_documents(pages)\n",
        "\n",
        "        # Û³. Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ (Vector Database)\n",
        "        vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "        # Û´. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· (Retrieval)\n",
        "        relevant_docs = vectorstore.similarity_search(user_query, k=3)\n",
        "        context = \" \".join([d.page_content for d in relevant_docs])\n",
        "\n",
        "        # Ûµ. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ (Generation)\n",
        "        prompt = f\"Answer the question using the context below.\\nContext: {context}\\nQuestion: {user_query}\"\n",
        "        result = qa_pipeline(prompt, max_length=150)\n",
        "\n",
        "        return result[0]['generated_text']\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}\"\n",
        "\n",
        "# --- Ø·Ø±Ø§Ø­ÛŒ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ (Gradio UI) ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(f\"\"\"\n",
        "    # ğŸ¤– Ø³ÛŒØ³ØªÙ… Ú†Øª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø²Ù‡Ø±Ø§ (Zahra's RAG System)\n",
        "    ### ØªÙˆØ³Ø¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ LangChainØŒ FAISS Ùˆ Transformers\n",
        "    Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒÚ© Ø³ÛŒØ³ØªÙ… **Retrieval-Augmented Generation** Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ø´Ù…Ø§ Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ø¨Ø§ Ù‡Ø± ÙØ§ÛŒÙ„ PDF Ú†Øª Ú©Ù†ÛŒØ¯.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            file_input = gr.File(label=\"Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ PDF\", file_types=[\".pdf\"])\n",
        "        with gr.Column(scale=2):\n",
        "            query_input = gr.Textbox(label=\"Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„ Ø¨Ù¾Ø±Ø³ÛŒØ¯:\", placeholder=\"Ù…Ø«Ù„Ø§Ù‹: What is the summary of this document?\")\n",
        "            output_text = gr.Textbox(label=\"Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ:\", interactive=False)\n",
        "\n",
        "    submit_btn = gr.Button(\"ØªØ­Ù„ÛŒÙ„ Ùˆ Ù¾Ø§Ø³Ø®Ø¯Ù‡ÛŒ\", variant=\"primary\")\n",
        "\n",
        "    # ØªØ¹Ø±ÛŒÙ Ø§Ú©Ø´Ù† Ø¯Ú©Ù…Ù‡\n",
        "    submit_btn.click(\n",
        "        fn=process_pdf_and_chat,\n",
        "        inputs=[file_input, query_input],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"--- \\n *Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø²Ù‡Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù‚Ø¯Ø±Øª RAG Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³Ù†Ø§Ø¯.*\")\n",
        "\n",
        "# Ø§Ø¬Ø±Ø§ (Ø§Ú¯Ø± share=True Ø¨Ø§Ø´Ø¯ØŒ Ù„ÛŒÙ†Ú© Ø¹Ù…ÙˆÙ…ÛŒ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯)\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "Gror4mi-YOHk",
        "outputId": "0bcb8b52-716f-4f37-db4d-7aa0fdaa5361"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n",
            "Loading LLM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/tmp/ipython-input-1652981425.py:69: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f9523693fa6a2e5a7b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f9523693fa6a2e5a7b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}